{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow_datasets as tfds\n",
    "#loading imdb reviews data. Needed only the first time.\n",
    "imdb, info = tfds.load('imdb_reviews',with_info=True, as_supervised=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data=imdb['train'], imdb['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_sentences=[]\n",
    "training_labels=[]\n",
    "testing_sentences=[]\n",
    "testing_labels=[]\n",
    "\n",
    "for s,l in train_data:\n",
    "    training_sentences.append(str(s.numpy()))\n",
    "    training_labels.append(l.numpy())\n",
    "for s,l in test_data:\n",
    "    testing_sentences.append(str(s.numpy()))\n",
    "    testing_labels.append(l.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_labels_final=np.array(training_labels)\n",
    "testing_labels_final=np.array(testing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Words:86539\n",
      "Word indices:[('movies', 101), ('any', 102), (\"it's\", 103), ('after', 104), ('think', 105)]\n",
      "\n",
      "\\Sentence:b\"This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting could not redeem this movie's ridiculous storyline. This movie is an early nineties US propaganda piece. The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions. Maria Conchita Alonso appeared phony, and her pseudo-love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning. I am disappointed that there are movies like this, ruining actor's like Christopher Walken's good name. I could barely sit through it.\"\n",
      "Sequence:[59, 12, 14, 35, 439, 400, 18, 174, 29, 1, 9, 33, 1378, 3401, 42, 496, 1, 197, 25, 88, 156, 19, 12, 211, 340, 29, 70, 248, 213, 9, 486, 62, 70, 88, 116, 99, 24, 5740, 12, 3317, 657, 777, 12, 18, 7, 35, 406, 8228, 178, 2477, 426, 2, 92, 1253, 140, 72, 149, 55, 2, 1, 7525, 72, 229, 70, 2962, 16, 1, 2880, 1, 1, 1506, 4998, 3, 40, 3947, 119, 1608, 17, 3401, 14, 163, 19, 4, 1253, 927, 7986, 9, 4, 18, 13, 14, 4200, 5, 102, 148, 1237, 11, 240, 692, 13, 44, 25, 101, 39, 12, 7232, 1, 39, 1378, 1, 52, 409, 11, 99, 1214, 874, 145, 10]\n",
      "\\Padded Sequence:[   0    0   59   12   14   35  439  400   18  174   29    1    9   33\n",
      " 1378 3401   42  496    1  197   25   88  156   19   12  211  340   29\n",
      "   70  248  213    9  486   62   70   88  116   99   24 5740   12 3317\n",
      "  657  777   12   18    7   35  406 8228  178 2477  426    2   92 1253\n",
      "  140   72  149   55    2    1 7525   72  229   70 2962   16    1 2880\n",
      "    1    1 1506 4998    3   40 3947  119 1608   17 3401   14  163   19\n",
      "    4 1253  927 7986    9    4   18   13   14 4200    5  102  148 1237\n",
      "   11  240  692   13   44   25  101   39   12 7232    1   39 1378    1\n",
      "   52  409   11   99 1214  874  145   10]\n"
     ]
    }
   ],
   "source": [
    "vocab_size=10000\n",
    "embedding_dim=16\n",
    "trunc_type='post'\n",
    "max_length=120\n",
    "oov_token='<OOV>'\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "tokenizer=Tokenizer(num_words=vocab_size,oov_token=oov_token)\n",
    "tokenizer.fit_on_texts(training_sentences)\n",
    "word_index=tokenizer.word_index\n",
    "\n",
    "vocabulary=len(list(word_index.items()))\n",
    "print(\"Unique Words:\"+str(vocabulary))\n",
    "print('Word indices:'+str([(k,v) for k,v in list(word_index.items())[100:105]])+'\\n')\n",
    "\n",
    "sequences=tokenizer.texts_to_sequences(training_sentences)\n",
    "print('\\Sentence:'+training_sentences[0]+'\\nSequence:'+str(sequences[0]))\n",
    "\n",
    "padded=pad_sequences(sequences,maxlen=max_length,truncating=trunc_type)\n",
    "print('\\Padded Sequence:'+ str(padded[0]))\n",
    "\n",
    "testing_sequences=tokenizer.texts_to_sequences(testing_sentences)\n",
    "testing_padded=pad_sequences(testing_sequences,maxlen=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "model=tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(6,activation='relu'),\n",
    "    tf.keras.layers.Dense(1,activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 120)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 120)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_padded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 120, 16)           160000    \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1920)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 6)                 11526     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 7         \n",
      "=================================================================\n",
      "Total params: 171,533\n",
      "Trainable params: 171,533\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/10\n",
      "25000/25000 [==============================] - 3s 113us/sample - loss: 0.5089 - accuracy: 0.7279 - val_loss: 0.3483 - val_accuracy: 0.8506\n",
      "Epoch 2/10\n",
      "25000/25000 [==============================] - 2s 91us/sample - loss: 0.2460 - accuracy: 0.9038 - val_loss: 0.3864 - val_accuracy: 0.8318\n",
      "Epoch 3/10\n",
      "25000/25000 [==============================] - 2s 95us/sample - loss: 0.1035 - accuracy: 0.9722 - val_loss: 0.4428 - val_accuracy: 0.8290\n",
      "Epoch 4/10\n",
      "25000/25000 [==============================] - 2s 100us/sample - loss: 0.0288 - accuracy: 0.9962 - val_loss: 0.5208 - val_accuracy: 0.8264\n",
      "Epoch 5/10\n",
      "25000/25000 [==============================] - 3s 104us/sample - loss: 0.0102 - accuracy: 0.9988 - val_loss: 0.5793 - val_accuracy: 0.8237\n",
      "Epoch 6/10\n",
      "25000/25000 [==============================] - 2s 96us/sample - loss: 0.0035 - accuracy: 0.9997 - val_loss: 0.6438 - val_accuracy: 0.8239\n",
      "Epoch 7/10\n",
      "25000/25000 [==============================] - 2s 92us/sample - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.6899 - val_accuracy: 0.8262\n",
      "Epoch 8/10\n",
      "25000/25000 [==============================] - 2s 94us/sample - loss: 5.4706e-04 - accuracy: 1.0000 - val_loss: 0.7287 - val_accuracy: 0.8267\n",
      "Epoch 9/10\n",
      "25000/25000 [==============================] - 3s 102us/sample - loss: 3.0935e-04 - accuracy: 1.0000 - val_loss: 0.7658 - val_accuracy: 0.8274\n",
      "Epoch 10/10\n",
      "25000/25000 [==============================] - 3s 101us/sample - loss: 1.8438e-04 - accuracy: 1.0000 - val_loss: 0.8029 - val_accuracy: 0.8268\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1346385d0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(padded, training_labels_final, epochs=10, validation_data=(testing_padded, testing_labels_final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 16)\n"
     ]
    }
   ],
   "source": [
    "embed_layer = model.layers[0]\n",
    "weights = embed_layer.get_weights()[0]\n",
    "print(weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
    "\n",
    "out_v = io.open('vecs.tsv', 'w', encoding='utf-8')\n",
    "out_m = io.open('meta.tsv', 'w', encoding='utf-8')\n",
    "for word_num in range(1, vocab_size):\n",
    "  word = reverse_word_index[word_num]\n",
    "  embeddings = weights[word_num]\n",
    "  out_m.write(word + \"\\n\")\n",
    "  out_v.write('\\t'.join([str(x) for x in embeddings]) + \"\\n\")\n",
    "out_v.close()\n",
    "out_m.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review:This movie was awesome. Really loved it. Sentiment:[0.9884066]\n",
      "\n",
      "Review:I almost fell asleep watching this. Sentiment:[0.13280706]\n",
      "\n",
      "Review:This movie is so stupid. Sentiment:[0.10917035]\n",
      "\n",
      "Review:I wouldn't watch this movie unless there's a gun pointed at me. Sentiment:[0.01397293]\n",
      "\n",
      "Review:This movie is so hilarious. Much love to the director! Sentiment:[0.93647313]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_sents = [\n",
    "    'This movie was awesome. Really loved it.',\n",
    "    'I almost fell asleep watching this.',\n",
    "    'This movie is so stupid.',\n",
    "    'I wouldn\\'t watch this movie unless there\\'s a gun pointed at me.',\n",
    "    'This movie is so hilarious. Much love to the director!'\n",
    "]\n",
    "new_seq = tokenizer.texts_to_sequences(new_sents)\n",
    "padded=pad_sequences(new_seq, maxlen=max_length,truncating=trunc_type)\n",
    "output=model.predict(padded)\n",
    "for i in range(0,len(new_sents)):\n",
    "    print('Review:'+new_sents[i]+' '+'Sentiment:'+str(output[i])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
